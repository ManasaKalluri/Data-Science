{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a TFIDF Vectorizer & compare its results with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm \n",
    "import math\n",
    "import scipy\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SkLearn Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(corpus):\n",
    "    '''\n",
    "    Function to get unique words in corpus which will be columns in output sparse matrix \n",
    "    \n",
    "    '''\n",
    "    unique_words = [] # Initialising empty list of unique words\n",
    "    for doc in corpus:\n",
    "        words = doc.split(\" \")\n",
    "        for word in words:\n",
    "            if word in unique_words:\n",
    "                continue\n",
    "            elif len(word)<2:   #To eliminate punctuations\n",
    "                continue\n",
    "            else:\n",
    "                unique_words.append(word)\n",
    "    return unique_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'document', 'second', 'and', 'third', 'one']\n"
     ]
    }
   ],
   "source": [
    "vocab = fit(corpus) \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus):\n",
    "    '''\n",
    "    Function to get IDF values of all unique words in corpus\n",
    "    '''\n",
    "    N = len(corpus)\n",
    "    IDF_dict = {}\n",
    "    for word in vocab:\n",
    "        count = 0\n",
    "        for doc in corpus:\n",
    "            if word in doc.split():\n",
    "                count += 1\n",
    "            IDF = (math.log((1+N)/(count+1))) +1\n",
    "            IDF_dict[word] = IDF\n",
    "            \n",
    "    return IDF_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 1.0,\n",
       " 'is': 1.0,\n",
       " 'the': 1.0,\n",
       " 'first': 1.5108256237659907,\n",
       " 'document': 1.2231435513142097,\n",
       " 'second': 1.916290731874155,\n",
       " 'and': 1.916290731874155,\n",
       " 'third': 1.916290731874155,\n",
       " 'one': 1.916290731874155}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Above IDF values are verified and same as Sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(corpus):\n",
    "    '''\n",
    "    Function to get TF values of unique words\n",
    "    '''\n",
    "    TF = {}\n",
    "    for row in range(len(corpus)):\n",
    "        word_list = corpus[row].split() \n",
    "        total_words_count = len(word_list)\n",
    "        word_counter = Counter(corpus[row].split())\n",
    "        for word in word_list:\n",
    "            if word in vocab:\n",
    "                TF[word] = word_counter[word] / total_words_count\n",
    "                \n",
    "    return TF\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0.2,\n",
       " 'is': 0.2,\n",
       " 'the': 0.2,\n",
       " 'first': 0.2,\n",
       " 'document': 0.2,\n",
       " 'second': 0.16666666666666666,\n",
       " 'and': 0.16666666666666666,\n",
       " 'third': 0.16666666666666666,\n",
       " 'one': 0.16666666666666666}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source : https://analyticsindiamag.com/hands-on-implementation-of-tf-idf-from-scratch-in-python/\n",
    "\n",
    "\n",
    "def transform(corpus):\n",
    "    '''\n",
    "    Function to get TFIDF output in sparse matrix representation \n",
    "    '''\n",
    "    sparse_matrix = lil_matrix((len(corpus), len(vocab)),dtype= np.float64) # Initialising sparse matrix with dimensions - corpus length and no.of unique words\n",
    "    for doc in range(len(corpus)):\n",
    "        for word in corpus[doc].split():\n",
    "            if word in vocab:\n",
    "                TFIDF_value = TF(corpus)[word] * IDF(corpus)[word]\n",
    "                sparse_matrix[doc,vocab.index(word)] = TFIDF_value \n",
    "    norm = normalize(sparse_matrix, norm = 'l2', axis=1,copy= True, return_norm = False) # Performing L2 normalization for output sparse matrix\n",
    "            \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.3840852409148149\n",
      "  (0, 1)\t0.3840852409148149\n",
      "  (0, 2)\t0.3840852409148149\n",
      "  (0, 3)\t0.580285823684436\n",
      "  (0, 4)\t0.4697913855799205\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing max features functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('F:\\Downloads\\cleaned_strings', 'rb') as f:\n",
    "    corpus_cleaned_strings = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus_cleaned_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cleaned_strings(corpus):\n",
    "    '''\n",
    "    Function to get unique words in corpus which will be columns in output sparse matrix \n",
    "    \n",
    "    '''\n",
    "    unique_words = [] # Initialising empty list of unique words\n",
    "    for doc in corpus:\n",
    "        words = doc.split(\" \")\n",
    "        for word in words:\n",
    "            if word in unique_words:\n",
    "                continue\n",
    "            elif len(word)<2:   #To eliminate punctuations\n",
    "                continue\n",
    "            else:\n",
    "                unique_words.append(word)\n",
    "    return unique_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2886\n"
     ]
    }
   ],
   "source": [
    "vocab_cleaned_strings = fit_cleaned_strings(corpus_cleaned_strings) \n",
    "print(len(vocab_cleaned_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF_cleaned_strings(corpus):\n",
    "    '''\n",
    "    Function to get IDF values of all unique words in corpus\n",
    "    '''\n",
    "    N = len(corpus)\n",
    "    IDF_dict = {}\n",
    "    for word in vocab_cleaned_strings:\n",
    "        count = 0\n",
    "        for doc in corpus:\n",
    "            if word in doc.split():\n",
    "                count += 1\n",
    "            IDF = (math.log((1+N)/(count+1))) +1\n",
    "            IDF_dict[word] = IDF\n",
    "    #Sorting and taking words with 50 highest IDF values\n",
    "    sorted_IDF_dict = {i: j for i, j in sorted(IDF_dict.items(), key=lambda item: item[1],reverse = True)[:50]}\n",
    "    \n",
    "    return sorted_IDF_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aimless': 6.922918004572872, 'distressed': 6.922918004572872, 'drifting': 6.922918004572872, 'nearly': 6.922918004572872, 'attempting': 6.922918004572872, 'artiness': 6.922918004572872, 'existent': 6.922918004572872, 'gerardo': 6.922918004572872, 'emptiness': 6.922918004572872, 'effort': 6.922918004572872, 'messages': 6.922918004572872, 'buffet': 6.922918004572872, 'science': 6.922918004572872, 'teacher': 6.922918004572872, 'baby': 6.922918004572872, 'owls': 6.922918004572872, 'florida': 6.922918004572872, 'muppets': 6.922918004572872, 'person': 6.922918004572872, 'overdue': 6.922918004572872, 'screenplay': 6.922918004572872, 'post': 6.922918004572872, 'practically': 6.922918004572872, 'structure': 6.922918004572872, 'tightly': 6.922918004572872, 'constructed': 6.922918004572872, 'vitally': 6.922918004572872, 'occurs': 6.922918004572872, 'content': 6.922918004572872, 'fill': 6.922918004572872, 'dozen': 6.922918004572872, 'highest': 6.922918004572872, 'superlative': 6.922918004572872, 'require': 6.922918004572872, 'puzzle': 6.922918004572872, 'solving': 6.922918004572872, 'fit': 6.922918004572872, 'pulls': 6.922918004572872, 'punches': 6.922918004572872, 'graphics': 6.922918004572872, 'number': 6.922918004572872, 'th': 6.922918004572872, 'insane': 6.922918004572872, 'massive': 6.922918004572872, 'unlockable': 6.922918004572872, 'properly': 6.922918004572872, 'aye': 6.922918004572872, 'rocks': 6.922918004572872, 'doomed': 6.922918004572872, 'conception': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "output = IDF_cleaned_strings(corpus_cleaned_strings)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aimless', 'distressed', 'drifting', 'nearly', 'attempting', 'artiness', 'existent', 'gerardo', 'emptiness', 'effort', 'messages', 'buffet', 'science', 'teacher', 'baby', 'owls', 'florida', 'muppets', 'person', 'overdue', 'screenplay', 'post', 'practically', 'structure', 'tightly', 'constructed', 'vitally', 'occurs', 'content', 'fill', 'dozen', 'highest', 'superlative', 'require', 'puzzle', 'solving', 'fit', 'pulls', 'punches', 'graphics', 'number', 'th', 'insane', 'massive', 'unlockable', 'properly', 'aye', 'rocks', 'doomed', 'conception']\n"
     ]
    }
   ],
   "source": [
    "top_vocab = list(output.keys()) #Taking list of unique words of corpus with top 50 IDF values\n",
    "print(top_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_cleaned_strings(corpus):\n",
    "    '''\n",
    "    Function to get TF values of unique words\n",
    "    '''\n",
    "    TF = {}\n",
    "    for row in range(len(corpus)):\n",
    "        word_list = corpus[row].split() \n",
    "        total_words_count = len(word_list)\n",
    "        word_counter = Counter(corpus[row].split())\n",
    "        for word in word_list:\n",
    "            if word in top_vocab:\n",
    "                TF[word] = word_counter[word] / total_words_count\n",
    "                \n",
    "    return TF\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aimless': 0.125,\n",
       " 'distressed': 0.125,\n",
       " 'drifting': 0.125,\n",
       " 'nearly': 0.1111111111111111,\n",
       " 'attempting': 0.05263157894736842,\n",
       " 'artiness': 0.05263157894736842,\n",
       " 'existent': 0.05263157894736842,\n",
       " 'gerardo': 0.1,\n",
       " 'emptiness': 0.1,\n",
       " 'effort': 0.1111111111111111,\n",
       " 'messages': 0.1111111111111111,\n",
       " 'buffet': 0.16666666666666666,\n",
       " 'science': 0.16666666666666666,\n",
       " 'teacher': 0.16666666666666666,\n",
       " 'baby': 0.3333333333333333,\n",
       " 'owls': 0.3333333333333333,\n",
       " 'florida': 0.125,\n",
       " 'muppets': 0.25,\n",
       " 'person': 0.125,\n",
       " 'overdue': 0.07692307692307693,\n",
       " 'screenplay': 0.07142857142857142,\n",
       " 'post': 0.07142857142857142,\n",
       " 'practically': 0.14285714285714285,\n",
       " 'structure': 0.002277904328018223,\n",
       " 'tightly': 0.002277904328018223,\n",
       " 'constructed': 0.002277904328018223,\n",
       " 'vitally': 0.002277904328018223,\n",
       " 'occurs': 0.002277904328018223,\n",
       " 'content': 0.002277904328018223,\n",
       " 'fill': 0.002277904328018223,\n",
       " 'dozen': 0.002277904328018223,\n",
       " 'highest': 0.002277904328018223,\n",
       " 'superlative': 0.002277904328018223,\n",
       " 'require': 0.002277904328018223,\n",
       " 'puzzle': 0.002277904328018223,\n",
       " 'solving': 0.002277904328018223,\n",
       " 'fit': 0.004555808656036446,\n",
       " 'pulls': 0.002277904328018223,\n",
       " 'punches': 0.002277904328018223,\n",
       " 'graphics': 0.00683371298405467,\n",
       " 'number': 0.002277904328018223,\n",
       " 'th': 0.002277904328018223,\n",
       " 'insane': 0.002277904328018223,\n",
       " 'massive': 0.00683371298405467,\n",
       " 'unlockable': 0.002277904328018223,\n",
       " 'properly': 0.002277904328018223,\n",
       " 'aye': 0.002277904328018223,\n",
       " 'rocks': 0.002277904328018223,\n",
       " 'doomed': 0.002277904328018223,\n",
       " 'conception': 0.002277904328018223}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_cleaned_strings(corpus_cleaned_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cleaned_strings(corpus):\n",
    "    '''\n",
    "    Function to get TFIDF output in sparse matrix representation \n",
    "    '''\n",
    "    sparse_matrix = lil_matrix((len(corpus), len(top_vocab)),dtype= np.float64) # Initialising sparse matrix with dimensions - corpus length and no.of unique words\n",
    "    for doc in range(len(corpus)):\n",
    "        for word in corpus[doc].split():\n",
    "            if word in top_vocab:\n",
    "                TFIDF_value = TF_cleaned_strings(corpus)[word] * IDF_cleaned_strings(corpus)[word]\n",
    "                sparse_matrix[doc,top_vocab.index(word)] = TFIDF_value \n",
    "    norm = normalize(sparse_matrix, norm = 'l2', axis=1,copy= True, return_norm = False) # Performing L2 normalization for output sparse matrix\n",
    "       \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= transform_cleaned_strings(corpus_cleaned_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5773502691896257\n",
      "  (0, 1)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n",
      "  (1, 3)\t1.0\n",
      "  (2, 4)\t0.5773502691896258\n",
      "  (2, 5)\t0.5773502691896258\n",
      "  (2, 6)\t0.5773502691896258\n",
      "  (4, 7)\t1.0\n",
      "  (5, 8)\t1.0\n",
      "  (7, 9)\t0.7071067811865475\n",
      "  (7, 10)\t0.7071067811865475\n",
      "  (9, 11)\t0.5773502691896257\n",
      "  (9, 12)\t0.5773502691896257\n",
      "  (9, 13)\t0.5773502691896257\n",
      "  (10, 14)\t0.7071067811865476\n",
      "  (10, 15)\t0.7071067811865476\n",
      "  (11, 16)\t1.0\n",
      "  (12, 17)\t1.0\n",
      "  (15, 18)\t1.0\n",
      "  (16, 19)\t1.0\n",
      "  (17, 20)\t0.7071067811865475\n",
      "  (17, 21)\t0.7071067811865475\n",
      "  (18, 22)\t1.0\n",
      "  (19, 23)\t0.14744195615489714\n",
      "  (19, 24)\t0.14744195615489714\n",
      "  (19, 25)\t0.14744195615489714\n",
      "  (19, 26)\t0.14744195615489714\n",
      "  (19, 27)\t0.14744195615489714\n",
      "  (19, 28)\t0.14744195615489714\n",
      "  (19, 29)\t0.14744195615489714\n",
      "  (19, 30)\t0.14744195615489714\n",
      "  (19, 31)\t0.14744195615489714\n",
      "  (19, 32)\t0.14744195615489714\n",
      "  (19, 33)\t0.14744195615489714\n",
      "  (19, 34)\t0.14744195615489714\n",
      "  (19, 35)\t0.14744195615489714\n",
      "  (19, 36)\t0.29488391230979427\n",
      "  (19, 37)\t0.14744195615489714\n",
      "  (19, 38)\t0.14744195615489714\n",
      "  (19, 39)\t0.44232586846469135\n",
      "  (19, 40)\t0.14744195615489714\n",
      "  (19, 41)\t0.14744195615489714\n",
      "  (19, 42)\t0.14744195615489714\n",
      "  (19, 43)\t0.44232586846469135\n",
      "  (19, 44)\t0.14744195615489714\n",
      "  (19, 45)\t0.14744195615489714\n",
      "  (19, 46)\t0.14744195615489714\n",
      "  (19, 47)\t0.14744195615489714\n",
      "  (19, 48)\t0.14744195615489714\n",
      "  (19, 49)\t0.14744195615489714\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5773502691896257\n",
      "  (0, 1)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n"
     ]
    }
   ],
   "source": [
    "output_one_doc = output[0]\n",
    "print(output_one_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_matrix = output_one_doc.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(dense_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
